{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec50ad5-8f48-4de5-8e1b-d3b53b59bd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89970a1c-36c4-4401-9b31-3da0bc66685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import time\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e825cd1-ef97-4b3d-87a0-1e3573d701cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3ab9b7e-40ab-468f-91d0-573ccccf4640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset at: C:\\Users\\Lenovo\\.keras\\datasets\\fra-eng_extracted\\fra.txt\n"
     ]
    }
   ],
   "source": [
    "#Download Dataset (English–French for example)\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'fra-eng.zip',\n",
    "    origin='http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip',\n",
    "    extract=True\n",
    ")\n",
    "\n",
    "# Dynamically get the full path to the unzipped file\n",
    "extracted_folder = os.path.splitext(path_to_zip)[0]  # removes .zip\n",
    "# path_to_file = os.path.join(os.path.dirname(path_to_zip), \"fra-eng\", \"fra.txt\")\n",
    "path_to_file = r\"C:\\Users\\Lenovo\\.keras\\datasets\\fra-eng_extracted\\fra.txt\"\n",
    "\n",
    "\n",
    "print(\"Using dataset at:\", path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b485b92d-fd55-4eb3-b68a-7bceee2ad91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Clean Data\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "def create_dataset(path, num_examples=None):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')[:2]] for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)\n",
    "\n",
    "en_sentences, fr_sentences = create_dataset(path_to_file, 30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aff0894-ee20-4ec0-956d-ff5d3331610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize and Pad\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, inp_lang = tokenize(fr_sentences)\n",
    "target_tensor, targ_lang = tokenize(en_sentences)\n",
    "\n",
    "max_length_inp = input_tensor.shape[1]\n",
    "max_length_targ = target_tensor.shape[1]\n",
    "\n",
    "input_tensor_train, target_tensor_train = input_tensor, target_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed0a3112-99a4-476a-9607-9a84c7c84730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Encoder & Decoder\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "# steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
    "steps_per_epoch = 50 # 100 or 50 for quick testing\n",
    "\n",
    "# embedding_dim = 256\n",
    "# units = 1024\n",
    "# To speed up reduce units\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "# dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "# To speed up training\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor))\n",
    "dataset = dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super().__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c88740f-9c03-4d42-9e2e-00be2368360e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43926ea0-da06-45ac-b3cd-0eb1cc8e351b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super().__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a244385-d892-422f-92e8-bc89fb779e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.9685, Time: 37.54s\n",
      "Epoch 2, Loss: 2.3750, Time: 26.41s\n",
      "Epoch 3, Loss: 2.1951, Time: 27.29s\n",
      "Epoch 4, Loss: 2.0886, Time: 27.15s\n",
      "Epoch 5, Loss: 1.9876, Time: 27.64s\n",
      "Epoch 6, Loss: 1.8746, Time: 27.22s\n",
      "Epoch 7, Loss: 1.8120, Time: 27.27s\n",
      "Epoch 8, Loss: 1.7449, Time: 27.19s\n",
      "Epoch 9, Loss: 1.6853, Time: 27.20s\n",
      "Epoch 10, Loss: 1.6369, Time: 27.27s\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    return tf.reduce_mean(loss_ * mask)\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / int(targ.shape[1])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     start = time.time()\n",
    "#     enc_hidden = encoder.initialize_hidden_state()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "#         batch_loss = train_step(inp, targ, enc_hidden)\n",
    "#         total_loss += batch_loss\n",
    "#     print(f'Epoch {epoch+1}, Loss: {total_loss/steps_per_epoch:.4f}, Time: {time.time()-start:.2f}s')\n",
    "\n",
    "@tf.function\n",
    "def run_epoch():\n",
    "    total_loss = 0.0\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "    return total_loss\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = run_epoch()\n",
    "    print(f'Epoch {epoch+1}, Loss: {total_loss/steps_per_epoch:.4f}, Time: {time.time()-start:.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e7a452d-4ecc-4007-8de6-14a3c5b228aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translation on User Input\n",
    "def evaluate(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang.word_index.get(i, 0) for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "    enc_hidden = tf.zeros((1, units))\n",
    "    enc_out, enc_hidden = encoder(inputs, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for _ in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word.get(predicted_id, '') + ' '\n",
    "\n",
    "        if targ_lang.index_word.get(predicted_id) == '<end>':\n",
    "            break\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result.strip()\n",
    "\n",
    "def translate(sentence):\n",
    "    result = evaluate(sentence)\n",
    "    print(f'Input: {sentence}')\n",
    "    print(f'Translated: {result}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9266e4e-b6c0-4db3-8555-f400309f2295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: bonjour\n",
      "Translated: it s a lot . <end>\n"
     ]
    }
   ],
   "source": [
    "# Try user input\n",
    "translate(\"bonjour\")   # Example French word input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9720b83f-f58b-4310-b568-f531e87c2929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e03e91-e288-49c1-a8cc-0ac83e182a77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16bd684-b21c-4a39-94a0-1283b2806bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f62d4-a185-43bb-8386-5c3d0d3ae4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830ba143-e099-4d30-8876-7dcf87fedc35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a6de0-7f95-477c-a0dc-17cd978f05e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb9db4-46de-44c1-b710-e74975ba0923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc8ef0-3b6c-402d-a0db-3327b5373e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275378f4-7f66-470e-9a91-4e18de8b0132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f0d4a-d878-4bbb-a735-23b42cf308b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
